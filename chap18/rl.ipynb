{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 18 -Reinforcement Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 18._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/18_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/18_reinforcement_learning.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And TensorFlow â‰¥ 2.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:55:23.864271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 20:55:26.726721: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bimri/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-05-20 20:55:26.726756: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-20 20:55:33.043616: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bimri/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-05-20 20:55:33.045317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bimri/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-05-20 20:55:33.045332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the default font sizes to make the figures prettier. We will also display some Matplotlib animations, and there are several possible options to do that: we will use the Javascript option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)\n",
    "plt.rc('animation', html='jshtml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create the `images/rl` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"rl\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter can be very slow without a GPU, so let's make sure there's one, or else issue a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. Neural nets can be very slow without a GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:55:48.324571: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bimri/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2023-05-20 20:55:48.334127: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-20 20:55:48.334268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (avtr): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install the gym library, which provides many environments for Reinforcement Learning. We'll also install the extra libraries needed for classic control environments (including CartPole, which we will use shortly), as well as for Box2D and Atari environments, which are needed for the exercises.\n",
    "\n",
    "**Note:** by running the following cell, you accept the Atari ROM license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
    "    %pip install -q -U gym\n",
    "    %pip install -q -U gym[classic_control,box2d,atari,accept-rom-license]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with. Let's import Gym and make a new CartPole environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CartPole (version 1) is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: `gym.envs.registry` is a dictionary containing all available environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acrobot-v1',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'Blackjack-v1',\n",
       " 'CarRacing-v2',\n",
       " 'CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " \"...there's more\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code - shows the first few environments\n",
    "envs = gym.envs.registry\n",
    "sorted(envs.keys())[:10] + [\"...there's more\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The registry values are environment specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code â€“ shows the specification for the CartPole-v1 environment\n",
    "envs[\"CartPole-v1\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the environment by calling is `reset()` method. This returns an observation, as well as a dictionary that may contain extra information. Both are environment-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the CartPole, each observation is a 1D NumPy array composed of 4 floats: they represent the cart's horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An environment can be visualized by calling its `render()` method. If you set `render_mode` to `\"rgb_array\"` when creating the environment, then this will return a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = env.render()\n",
    "img.shape  # height, width, channels (3 = Red, Green, Blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ creates a little function to render and plot an environment\n",
    "\n",
    "def plot_environment(env, figsize=(5, 4)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img\n",
    "\n",
    "plot_environment(env)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to interact with an environment. Your agent will need to select an action from an \"action space\" (the set of possible actions). Let's see what this environment's action space looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, just two possible actions: accelerate towards the left (0) or towards the right (1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pole is leaning toward the right (`obs[2] > 0`), let's accelerate the cart toward the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1  # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "obs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cart is now moving toward the right (`obs[1] > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the left after the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ displays the environment\n",
    "plot_environment(env)\n",
    "save_fig(\"cart_pole_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's doing what we're telling it to do!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment also tells the agent how much reward it got during the last step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some environment wrappers may want to interrupt the environment early. For example, when a time limit is reached or when an object goes out of bounds. In this case, `truncated` will be set to `True`. In this case, it's not truncated yet:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `info` is an environment-specific dictionary that can provide some extra information that you may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of steps between the moment the environment is reset until it is done or truncated is called an \"episode\". At the end of an episode (i.e., when `step()` returns `done=True` or `truncated=True`), you should reset the environment before you continue to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if done or truncated:\n",
    "    obs, info = env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how can we make the poll remain upright? We will need to define a _policy_ for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple hard-coded policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.698, 8.389445512070509, 24.0, 63.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as expected, this strategy is a bit too basic: the best it did was to keep the poll up for only 63 steps. This environment is considered solved when the agent keeps the poll up for 200 steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one episode. You can learn more about Matplotlib animations in the [Matplotlib tutorial notebook](tools_matplotlib.ipynb#Animations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ this cell displays an animation of one episode\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = matplotlib.animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "def show_one_episode(policy, n_max_steps=200, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    np.random.seed(seed)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(env.render())\n",
    "        action = policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    env.close()\n",
    "    return plot_animation(frames)\n",
    "\n",
    "show_one_episode(basic_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the system is unstable and after just a few wobbles, the pole ends up too tilted: game over. We will need to be smarter than that!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network that will take observations as inputs, and output the probabilities of actions to take for each observation. To choose an action, the network will estimate a probability for each action, then we will select an action randomly according to the estimated probabilities. In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 0 (left), and of course the probability of action 1 (right) will be `1 - p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:58:32.980279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)  # ensures reproducibility on the CPU\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why we plan to pick a random action based on the probability given by the policy network, rather than just picking the action with the highest probability. This approach lets the agent find the right balance between _exploring_ new actions and _exploiting_ the actions that are known to work well. Here's an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn't increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ a function that creates an animation for a given policy model\n",
    "\n",
    "def pg_policy(obs):\n",
    "    left_proba = model.predict(obs[np.newaxis], verbose=0)\n",
    "    return int(np.random.rand() > left_proba)\n",
    "\n",
    "np.random.seed(42)\n",
    "show_one_episode(pg_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah... pretty bad. The neural network will have to learn to do better. First let's see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it can learn a better policy on its own. One that does not wobble as much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this neural network we will need to define the target probabilities **y**. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. But how do we know whether an action is good or bad? The problem is that most actions have delayed effects, so when you win or lose points in an episode, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? This is called the _credit assignment problem_.\n",
    "\n",
    "The _Policy Gradients_ algorithm tackles this problem by first playing multiple episodes, then making the actions near positive rewards slightly more likely, while actions near negative rewards are made slightly less likely. First we play, then we go back and think about what we did."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a function to play a single step using the model. We will also pretend for now that whatever action it takes is the right one, so we can compute the loss and its gradients. We will just save these gradients for now, and modify them later depending on how good or bad the action turned out to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `left_proba` is high, then `action` will most likely be `False` (since a random number uniformally sampled between 0 and 1 will probably not be greater than `left_proba`). And `False` means 0 when you cast it to a number, so `y_target` would be equal to 1 - 0 = 1. In other words, we set the target to 1, meaning we pretend that the probability of going left should have been 100% (so we took the right action)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients, for each episode and each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Policy Gradients algorithm uses the model to play the episode several times (e.g., 10 times), then it goes back and looks at all the rewards, discounts them and normalizes them. So let's create couple functions for that: the first will compute discounted rewards; the second will normalize the discounted rewards across many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards, and we subtract the mean from each discounted reward, and divide by the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "                               discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ let's create the neural net and reset the environment, for\n",
    "#              reproducibility\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "obs, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 150/150, mean rewards: 197.6"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "\n",
    "    # extra code â€“ displays some debug info during training\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
    "          f\" mean rewards: {total_rewards / n_episodes_per_update:.1f}\", end=\"\")\n",
    "\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_factor)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ displays the animation\n",
    "np.random.seed(42)\n",
    "show_one_episode(pg_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Material â€“ Markov Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following transition probabilities correspond to the Markov Chain represented in Figure 18â€“7. Let's run this stochastic process a few times to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1: 0 0 3 \n",
      "Run #2: 0 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #3: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #4: 0 3 \n",
      "Run #5: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #6: 0 1 3 \n",
      "Run #7: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #8: 0 0 0 1 2 1 2 1 3 \n",
      "Run #9: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #10: 0 0 0 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to s0, s1, s2, s3\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to s0, s1, s2, s3\n",
    "\n",
    "n_max_steps = 1000  # to avoid blocking in case of an infinite loop\n",
    "terminal_states = [3]\n",
    "\n",
    "def run_chain(start_state):\n",
    "    current_state = start_state\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state in terminal_states:\n",
    "            break\n",
    "        current_state = np.random.choice(\n",
    "            range(len(transition_probabilities)),\n",
    "            p=transition_probabilities[current_state]\n",
    "        )\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "\n",
    "    print()\n",
    "\n",
    "for idx in range(10):\n",
    "    print(f\"Run #{idx + 1}: \", end=\"\")\n",
    "    run_chain(start_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some transition probabilities, rewards and possible actions. For example, in state s0, if action a0 is chosen then with proba 0.7 we will go to state s0 with reward +10, with probability 0.3 we will go to state s1 with no reward, and with never go to state s2 (so the transition probabilities are `[0.7, 0.3, 0.0]`, and the rewards are `[+10, 0, 0]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [  # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "rewards = [  # shape=[s, a, s']\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf)  # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90  # the discount factor\n",
    "\n",
    "history1 = []  # extra code â€“ needed for the figure below\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    history1.append(Q_prev) # extra code\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "                for sp in range(3)])\n",
    "\n",
    "history1 = np.array(history1)  # extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values.argmax(axis=1)  # optimal action for each state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy for this MDP, when using a discount factor of 0.90, is to choose action a0 when in state s0, and choose action a0 when in state s1, and finally choose action a1 (the only possible action) when in state s2. If you try again with a discount factor of 0.95 instead of 0.90, you will find that the optimal action for state s1 becomes a2. This is because the discount factor is larger so the agent values the future more, and it is therefore ready to pay an immediate penalty in order to get more future rewards."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to simulate an agent moving around in the environment, so let's define a function to perform some action and get the new state and a reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an exploration policy, which can be any policy, as long as it visits every possible state many times. We will just use a random policy, since the state space is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the Q-Values like earlier, and run the Q-Learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ initializes the Q-Values, just like earlier\n",
    "np.random.seed(42)\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state][actions] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05  # initial learning rate\n",
    "decay = 0.005  # learning rate decay\n",
    "gamma = 0.90  # discount factor\n",
    "state = 0  # initial state\n",
    "history2 = []  # extra code â€“ needed for the figure below\n",
    "\n",
    "for iteration in range(10_000):\n",
    "    history2.append(Q_values.copy())  # extra code\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = Q_values[next_state].max()  # greedy policy at the next step\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state\n",
    "\n",
    "history2 = np.array(history2)  # extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extra code â€“ this cell generates and saves Figure 18â€“9\n",
    "\n",
    "true_Q_value = history1[-1, 0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].set_ylabel(\"Q-Value$(s_0, a_0)$\", fontsize=14)\n",
    "axes[0].set_title(\"Q-Value Iteration\", fontsize=14)\n",
    "axes[1].set_title(\"Q-Learning\", fontsize=14)\n",
    "for ax, width, history in zip(axes, (50, 10000), (history1, history2)):\n",
    "    ax.plot([0, width], [true_Q_value, true_Q_value], \"k--\")\n",
    "    ax.plot(np.arange(width), history[:, 0, 0], \"b-\", linewidth=2)\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax.axis([0, width, 0, 24])\n",
    "    ax.grid(True)\n",
    "\n",
    "save_fig(\"q_value_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the DQN. Given a state, it will estimate, for each possible action, the sum of discounted future rewards it can expect after it plays that action (but before it sees its outcome):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # ensures reproducibility on the CPU\n",
    "\n",
    "input_shape = [4]  # == env.observation_space.shape\n",
    "n_outputs = 2  # == env.action_space.n\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select an action using this DQN, we just pick the action with the largest predicted Q-value. However, to ensure that the agent explores the environment, we choose a random action with probability `epsilon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)  # random action\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return Q_values.argmax()  # optimal action according to the DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a replay buffer. It will contain the agent's experiences, in the form of tuples: `(obs, action, reward, next_obs, done)`. We can use the `deque` class for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: for very large replay buffers, you may want to use a circular buffer instead, as random access time will be O(1) instead of O(N). Or you can check out DeepMind's [Reverb library](https://github.com/deepmind/reverb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ A basic circular buffer implementation\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = np.empty(max_size, dtype=object)\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.randint(self.size, size=batch_size)\n",
    "        return self.buffer[indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create a function to sample experiences from the replay buffer. It will return 6 NumPy arrays: `[obs, actions, rewards, next_obs, dones, truncateds]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ]  # [states, actions, rewards, next_states, dones, truncateds]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a function that will use the DQN to play one step, and record its experience in the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's create a function that will sample some experiences from the replay buffer and perform a training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ for reproducibility, and to generate the next figure\n",
    "env.reset(seed=42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "rewards = [] \n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    obs, info = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # extra code â€“ displays debug info, stores data for the next figure, and\n",
    "    #              keeps track of the best model weights so far\n",
    "    print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
    "          end=\"\")\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights)  # extra code â€“ restores the best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ this cell generates and saves Figure 18â€“10\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "save_fig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ shows an animation of the trained DQN playing one episode\n",
    "show_one_episode(epsilon_greedy_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all! ðŸ˜€"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Q-Value Targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the online DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ creates the same DQN model as earlier\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the target DQN: it's just a clone of the online DQN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18867/1363552219.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clone the model's architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# copy the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "target = tf.keras.models.clone_model(model)  # clone the model's architecture\n",
    "target.set_weights(model.get_weights())  # copy the weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the same code as above except for the line marked with `# <= CHANGED`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(seed=42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)  # resets the replay buffer\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = target.predict(next_states, verbose=0)  # <= CHANGED\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is the same code as earlier, except for the lines marked with `# <= CHANGED`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    obs, info = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info, truncated = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # extra code â€“ displays debug info, stores data for the next figure, and\n",
    "    #              keeps track of the best model weights so far\n",
    "    print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
    "          end=\"\")\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:                        # <= CHANGED\n",
    "            target.set_weights(model.get_weights())  # <= CHANGED\n",
    "\n",
    "    # Alternatively, you can do soft updates at each step:\n",
    "    #if episode > 50:\n",
    "        #training_step(batch_size)\n",
    "        #target_weights = target.get_weights()\n",
    "        #online_weights = model.get_weights()\n",
    "        #for index, online_weight in enumerate(online_weights):\n",
    "        #    target_weights[index] = (0.99 * target_weights[index]\n",
    "        #                             + 0.01 * online_weight)\n",
    "        #target.set_weights(target_weights)\n",
    "\n",
    "model.set_weights(best_weights)  # extra code â€“ restores the best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ this cell plots the learning curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ shows an animation of the trained DQN playing one episode\n",
    "show_one_episode(epsilon_greedy_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is exactly the same as for fixed Q-Value targets, except for the section marked as changed in the `training_step()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "target = tf.keras.models.clone_model(model)  # clone the model's architecture\n",
    "target.set_weights(model.get_weights())  # copy the weights\n",
    "\n",
    "env.reset(seed=42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "\n",
    "    #################### CHANGED SECTION ####################\n",
    "    next_Q_values = model.predict(next_states, verbose=0)  # â‰  target.predict()\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n",
    "                        ).sum(axis=1)\n",
    "    #########################################################\n",
    "\n",
    "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "\n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, info, truncated = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}\",\n",
    "          end=\"\")\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "        if episode % 50 == 0:\n",
    "            target.set_weights(model.get_weights())\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ this cell plots the learning curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code â€“ shows an animation of the trained DQN playing one episode\n",
    "show_one_episode(epsilon_greedy_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
